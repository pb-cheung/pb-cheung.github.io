(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{494:function(t,a,r){"use strict";r.r(a);var s=r(56),v=Object(s.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"大语言模型"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#大语言模型"}},[t._v("#")]),t._v(" 大语言模型")]),t._v(" "),r("p",[t._v("LLM：Large Language Model")]),t._v(" "),r("h2",{attrs:{id:"大模型的问题"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#大模型的问题"}},[t._v("#")]),t._v(" 大模型的问题")]),t._v(" "),r("h2",{attrs:{id:"幻觉"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#幻觉"}},[t._v("#")]),t._v(" 幻觉")]),t._v(" "),r("p",[t._v("即“一本正经的胡说八道”。\n造成这种问题的可能原因有以下几点：")]),t._v(" "),r("ol",[r("li",[t._v("大模型特点决定。它是基于概率的推理，大模型学习的是词语、概念之间的统计学关联性，它的训练目标是生成通顺符合语境的文本，并不能确保真实性。逐词生成的方式可能导致错误累积，逐步生成偏离事实的结果。")]),t._v(" "),r("li",[t._v("训练数据质量差。数据来自互联网，可能包含很多错误的不完整的甚至虚假的内容，缺乏非公开的垂直领域的数据。")]),t._v(" "),r("li",[t._v("提示词质量差。用户提供的的问题、提示词或者上下文信息可能表述不清楚或者错误，会诱导大模型走向错误。（投喂错误的实时信息造成污染）")])]),t._v(" "),r("h3",{attrs:{id:"幻觉问题改善"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#幻觉问题改善"}},[t._v("#")]),t._v(" 幻觉问题改善")]),t._v(" "),r("p",[t._v("大模型幻觉问题不能消除（因为原因1.和2.是模型自身的特点），可以改善。")]),t._v(" "),r("h2",{attrs:{id:"没有记忆"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#没有记忆"}},[t._v("#")]),t._v(" 没有记忆")]),t._v(" "),r("p",[t._v("每一次请求都是独立的（类似于HTTP请求）。")]),t._v(" "),r("h3",{attrs:{id:"多轮对话"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#多轮对话"}},[t._v("#")]),t._v(" 多轮对话")]),t._v(" "),r("p",[t._v("那么多轮的聊天对话（看起来像是有记忆）是怎么实现的呢？")]),t._v(" "),r("p",[t._v("每次新的对话开始，大模型会创建一个新的上下文（Context），在结束本对话前一直维持这个上下文。")]),t._v(" "),r("p",[t._v("之后的每轮对话，大模型会把之前所有轮的对话信息（问+答）放到上下文中，新一轮的问题的回答会结合最新的问题和上下文来生成。")])])}),[],!1,null,null,null);a.default=v.exports}}]);